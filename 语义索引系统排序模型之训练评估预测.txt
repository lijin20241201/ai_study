from functools import partial

import os
import random
import time

import numpy as np
import paddle
import paddle.nn.functional as F

from paddlenlp.data import Stack, Tuple, Pad
from paddlenlp.datasets import load_dataset
from paddlenlp.transformers import LinearDecayWithWarmup, AutoModel, AutoTokenizer

import pandas as pd
from tqdm import tqdm

margin=0.2
train_file="./datasets/yysy/sort/train_pairwise_s.csv"
test_file='./datasets/yysy/sort/dev_pairwise_s.csv'
save_dir='./checkpoint'
max_seq_length=128
batch_size=16
learning_rate=5e-5
weight_decay=0.0
epochs=2
warmup_proportion=0.0
init_from_ckpt=None
model_name_or_path='ernie-3.0-medium-zh'
seed=1000
device='gpu'

def set_seed(seed):
    """sets random seed"""
    random.seed(seed)
    np.random.seed(seed)
    paddle.seed(seed)

paddle.set_device(device)

set_seed(seed)

# 构建读取函数，读取原始数据
def read(src_path, is_predict=False):
    data = pd.read_csv(src_path, sep='\t')
    for index, row in tqdm(data.iterrows()):
        query = row['query']
        title = row['title']
        neg_title = row['neg_title']
        yield {'query': query, 'title': title, 'neg_title': neg_title}

train_ds = load_dataset(read, src_path=train_file, lazy=False)

a={'query': '英语委婉语引起的跨文化交际障碍',
 'title': '英语委婉语引起的跨文化交际障碍及其翻译策略研究英语委婉语,跨文化交际障碍,翻译策略',
 'neg_title': '委婉语在英语和汉语中的文化差异委婉语,文化,跨文化交际'}
len(a)

[i for i in train_ds if len(i)!=3]

def read_test(src_path, is_predict=False):
    data = pd.read_csv(src_path, sep='\t')
    for index, row in tqdm(data.iterrows()):
        query = row['query']
        title = row['title']
        label = row['label']
        yield {'query': query, 'title': title, 'label': label}

dev_ds = load_dataset(read_test, src_path=test_file, lazy=False)

[i for i in dev_ds if i['label']==1][:3]

pretrained_model = AutoModel.from_pretrained(model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

def convert_pairwise_example(example,
                             tokenizer,
                             max_seq_length=512,
                             phase="train"):

    if phase == "train":
        query, pos_title, neg_title = example["query"], example[
            "title"], example["neg_title"]

        pos_inputs = tokenizer(text=query,#正样本
                               text_pair=pos_title,
                               max_seq_len=max_seq_length)
        neg_inputs = tokenizer(text=query,#负样本
                               text_pair=neg_title,
                               max_seq_len=max_seq_length)

        pos_input_ids = pos_inputs["input_ids"]
        pos_token_type_ids = pos_inputs["token_type_ids"]
        neg_input_ids = neg_inputs["input_ids"]
        neg_token_type_ids = neg_inputs["token_type_ids"]

        return (pos_input_ids, pos_token_type_ids, neg_input_ids,
                neg_token_type_ids)

    else:#不是训练模式
        query, title = example["query"], example["title"]

        inputs = tokenizer(text=query,
                           text_pair=title,
                           max_seq_len=max_seq_length)

        input_ids = inputs["input_ids"]
        token_type_ids = inputs["token_type_ids"]
        if phase == "eval":
            return input_ids, token_type_ids, example["label"]
        elif phase == "predict":
            return input_ids, token_type_ids
        else:
            raise ValueError("not supported phase:{}".format(phase))

trans_func_train = partial(convert_pairwise_example,
                               tokenizer=tokenizer,
                               max_seq_length=max_seq_length)

trans_func_eval = partial(convert_pairwise_example,
                              tokenizer=tokenizer,
                              max_seq_length=max_seq_length,
                              phase="eval")

batchify_fn_train = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype="int64"
            ),  # pos_pair_input
        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype="int64"
            ),  # pos_pair_segment
        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype="int64"
            ),  # neg_pair_input
        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype="int64"
            )  # neg_pair_segment
    ): [data for data in fn(samples)]

batchify_fn_eval = lambda samples, fn=Tuple(
    Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype="int64"
        ),  # pair_input
    Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype="int64"
        ),  # pair_segment
    Stack(dtype="int64")  # label
): [data for data in fn(samples)]

def create_dataloader(dataset,
                      mode='train',
                      batch_size=1,
                      batchify_fn=None,
                      trans_fn=None):
    if trans_fn:# 转换成数字形式
        dataset = dataset.map(trans_fn)

    shuffle = True if mode == 'train' else False
    if mode == 'train':#批次样本
        batch_sampler = paddle.io.DistributedBatchSampler(dataset,
                                                          batch_size=batch_size,
                                                          shuffle=shuffle)
    else:
        batch_sampler = paddle.io.BatchSampler(dataset,
                                               batch_size=batch_size,
                                               shuffle=shuffle)

    return paddle.io.DataLoader(dataset=dataset,
                                batch_sampler=batch_sampler,
                                collate_fn=batchify_fn,
                                return_list=True)

train_data_loader = create_dataloader(train_ds,
                                          mode='train',
                                          batch_size=batch_size,
                                          batchify_fn=batchify_fn_train,
                                          trans_fn=trans_func_train)

dev_data_loader = create_dataloader(dev_ds,
                                    mode='dev',
                                    batch_size=batch_size,
                                    batchify_fn=batchify_fn_eval,
                                    trans_fn=trans_func_eval)

for i in train_data_loader:
    print(i)
    break

import paddle.nn as nn
import paddle.nn.functional as F

class PairwiseMatching(nn.Layer):
    def __init__(self, pretrained_model, dropout=None, margin=0.1):
        super().__init__()
        self.ptm = pretrained_model
        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)
        self.margin = margin
        # hidden_size -> 1, calculate similarity
        self.similarity = nn.Linear(self.ptm.config["hidden_size"], 1)

    @paddle.jit.to_static(input_spec=[
        paddle.static.InputSpec(shape=[None, None], dtype='int64'),
        paddle.static.InputSpec(shape=[None, None], dtype='int64')
    ])
    def predict(self,
                input_ids,
                token_type_ids=None,
                position_ids=None,
                attention_mask=None):

        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,
                                    attention_mask)

        cls_embedding = self.dropout(cls_embedding)#(n,d),池化输出
        sim_score = self.similarity(cls_embedding)#(n,1)
        sim_score = F.sigmoid(sim_score)#相似概率

        return sim_score

    def forward(self,
                pos_input_ids,
                neg_input_ids,
                pos_token_type_ids=None,
                neg_token_type_ids=None,
                pos_position_ids=None,
                neg_position_ids=None,
                pos_attention_mask=None,
                neg_attention_mask=None):

        _, pos_cls_embedding = self.ptm(pos_input_ids, pos_token_type_ids,#正样本嵌入(n,d)
                                        pos_position_ids, pos_attention_mask)

        _, neg_cls_embedding = self.ptm(neg_input_ids, neg_token_type_ids,#负样本嵌入(n,d)
                                        neg_position_ids, neg_attention_mask)

        pos_embedding = self.dropout(pos_cls_embedding)
        neg_embedding = self.dropout(neg_cls_embedding)

        pos_sim = self.similarity(pos_embedding)#(n,1)
        neg_sim = self.similarity(neg_embedding)#(n,1)

        pos_sim = F.sigmoid(pos_sim)#概率
        neg_sim = F.sigmoid(neg_sim)

        labels = paddle.full(shape=[pos_cls_embedding.shape[0]],#
                             fill_value=1.0,
                             dtype='float32')

        loss = F.margin_ranking_loss(pos_sim,
                                     neg_sim,
                                     labels,
                                     margin=self.margin)

        return loss

# 正样本相似度是指模型计算出的正样本对之间的相似度。正样本对通常指的是在某种意义下“相关”或“匹配”的数据点。
# 例如，在图像检索中，正样本对可能是一个查询图像和与之相关的另一个图像；在自然语言处理中，正样本对可能是一个问题和一个正确的答案。
# 负样本相似度是指模型计算出的负样本对之间的相似度。负样本对通常指的是在某种意义下“不相关”或“不匹配”的数据点。继续上面的例子，
# 在图像检索中，负样本对可能是一个查询图像和一个与之不相关的图像；在自然语言处理中，
# 负样本对可能是一个问题和一个错误的答案。

# 正样本相似度和负样本相似度在训练模型时非常重要，因为它们决定了模型如何学习区分相关和不相关的数据点。通过优化
# 一个损失函数（如margin_ranking_loss），模型可以学会将正样本的相似度最大化，同时将负样本的相似度最小化。这有助于模型在预测
# 时更准确地区分相关和不相关的数据点。

labels = paddle.full(shape=[6,],fill_value=1.0,dtype='float32')

model = PairwiseMatching(pretrained_model, margin=margin)#包装预训练模型的自定义下游模型

if init_from_ckpt and os.path.isfile(init_from_ckpt):
        state_dict = paddle.load(init_from_ckpt)
        model.set_dict(state_dict)
        print('从{}加载成功!'.format(init_from_ckpt))

num_training_steps = len(train_data_loader) * epochs

lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,
                                     warmup_proportion)

# Generate parameter names needed to perform weight decay.
# All bias and LayerNorm parameters are excluded.
decay_params = [
    p.name for n, p in model.named_parameters()
    if not any(nd in n for nd in ["bias", "norm"])
]
optimizer = paddle.optimizer.AdamW(
    learning_rate=lr_scheduler,
    parameters=model.parameters(),
    weight_decay=weight_decay,
    apply_decay_param_fun=lambda x: x in decay_params)

# AUC 是一种用于评估二分类问题模型性能的重要指标，它表示 ROC 曲线下的面积。ROC
# 曲线描绘了真正例率（TPR，True Positive Rate）和假正例率（FPR，False Positive
# Rate）之间的关系，随着分类阈值的变化而变化。AUC 值越接近 1，模型的性能越好，
# 表示模型能够很好地区分正负样本。

metric = paddle.metric.Auc()

def train(model,dataloader,optimizer,scheduler,global_step):
    model.train()
    total_loss=0.
    for step, batch in enumerate(dataloader, start=1):
        pos_input_ids, pos_token_type_ids, neg_input_ids, neg_token_type_ids = batch
        loss = model(pos_input_ids=pos_input_ids,
                         neg_input_ids=neg_input_ids,
                         pos_token_type_ids=pos_token_type_ids,
                         neg_token_type_ids=neg_token_type_ids)

        loss.backward()#反向传播
        optimizer.step()#梯度更新
        optimizer.clear_grad()#梯度归0
        scheduler.step()#更新学习率
        global_step += 1
        total_loss += loss.item()
        if global_step % 30 == 0 and global_step!=0:
            print("global_step %d - batch: %d -loss: %.5f"
                  %(global_step,step,loss.item()))
    avg_loss=total_loss/len(dataloader)
    return avg_loss,global_step

# AUC是一个重要的分类模型评价指标，因为它考虑了分类器对于正例和负例的分类能力，尤其在不平衡的数据集上表现出色。

# preds (numpy.array)：一个numpy数组，形状为(batch_size, 2)。preds[i][j]表示将实例i分类到类别j的概率。
# 这里，j的取值为0或1，分别代表负类和正类。
# labels (numpy.array)：一个numpy数组，形状为(batch_size, 1)。labels[i]是0或1，代表实例i的标签。
@paddle.no_grad()
def evaluate(model, metric, data_loader, phase="dev"):
    model.eval()
    metric.reset()#重置AUC 度量指标
    for idx, batch in enumerate(data_loader):
        input_ids,token_type_ids,labels = batch
        pos_probs = model.predict(input_ids=input_ids,
                                  token_type_ids=token_type_ids)
        neg_probs = 1.0 - pos_probs
        preds = np.concatenate((neg_probs, pos_probs), axis=1)# 每行包含两个概率值（负样本和正样本）
        metric.update(preds=preds, labels=labels)
        auc=metric.accumulate()
        return phase,auc

def train_epochs(epochs,save_path):
    global_step = 0#全局步
    best_auc=0.0#最好的损失
    for epoch in range(1,epochs+1):
        avg_loss,global_step = train(model,train_data_loader,optimizer,lr_scheduler,global_step)
        print("epoch:%d - global_step:%d - avg_loss: %.4f - best_auc: %.4f -lr:%.8f" \
          % (epoch,global_step,avg_loss,best_auc,optimizer.get_lr()))
        phase,auc=evaluate(model, metric, dev_data_loader)
        print("eval_{} auc:{:.3f}".format(phase,auc))
        if auc>best_auc:#avg_loss< best_loss才会更新保存模型
            paddle.save(model.state_dict(),\
                save_path+'yysy_best_1.pdparams')
            tokenizer.save_pretrained(save_path)
            best_auc=auc#更新best_auc

save_dir=save_dir+'/yxsy/'

if not os.path.exists(save_dir): os.makedirs(save_dir)
train_epochs(epochs,save_dir)

from functools import partial
import os
import random
import time

import numpy as np
import paddle
import paddle.nn.functional as F

from paddlenlp.data import Stack, Tuple, Pad
from paddlenlp.datasets import load_dataset
from paddlenlp.transformers import LinearDecayWithWarmup, AutoModel, AutoTokenizer
import pandas as pd
from tqdm import tqdm
import paddle.nn as nn

import sys
import os

device='gpu'
seed=1000
test_file='./datasets/yysy/sort/dev_pairwise_s.csv'
model_name_or_path='ernie-3.0-medium-zh'
max_seq_length=128
batch_size=32

paddle.set_device(device)

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    paddle.seed(seed)

set_seed(seed)

def read_test(src_path, is_predict=False):
    data = pd.read_csv(src_path, sep='\t')
    for index, row in tqdm(data.iterrows()):
        query = row['query']
        title = row['title']
        label = row['label']
        yield {'query': query, 'title': title, 'label': label}

dev_ds = load_dataset(read_test, src_path=test_file, lazy=False)

dev_ds[:5]

pretrained_model = AutoModel.from_pretrained(model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

def convert_pairwise_example(example,
                             tokenizer,
                             max_seq_length=512,
                             phase="train"):

    if phase == "train":
        query, pos_title, neg_title = example["query"], example[
            "title"], example["neg_title"]

        pos_inputs = tokenizer(text=query,
                               text_pair=pos_title,
                               max_seq_len=max_seq_length)
        neg_inputs = tokenizer(text=query,
                               text_pair=neg_title,
                               max_seq_len=max_seq_length)

        pos_input_ids = pos_inputs["input_ids"]
        pos_token_type_ids = pos_inputs["token_type_ids"]
        neg_input_ids = neg_inputs["input_ids"]
        neg_token_type_ids = neg_inputs["token_type_ids"]

        return (pos_input_ids, pos_token_type_ids, neg_input_ids,
                neg_token_type_ids)

    else:
        query, title = example["query"], example["title"]

        inputs = tokenizer(text=query,
                           text_pair=title,
                           max_seq_len=max_seq_length)

        input_ids = inputs["input_ids"]
        token_type_ids = inputs["token_type_ids"]
        if phase == "eval":
            return input_ids, token_type_ids, example["label"]
        elif phase == "predict":
            return input_ids, token_type_ids
        else:
            raise ValueError("not supported phase:{}".format(phase))

trans_func_eval = partial(convert_pairwise_example,
                              tokenizer=tokenizer,
                              max_seq_length=max_seq_length,
                              phase="eval")

batchify_fn_eval = lambda samples, fn=Tuple(
    Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype="int64"#input_ids
        ),  # pair_input
    Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype="int64"#token_type_ids
        ),  # pair_segment
    Stack(dtype="int64")  # label
): [data for data in fn(samples)]

def create_dataloader(dataset,
                      mode='train',
                      batch_size=1,
                      batchify_fn=None,
                      trans_fn=None):
    if trans_fn:
        dataset = dataset.map(trans_fn)

    shuffle = True if mode == 'train' else False
    if mode == 'train':
        batch_sampler = paddle.io.DistributedBatchSampler(dataset,
                                                          batch_size=batch_size,
                                                          shuffle=shuffle)
    else:
        batch_sampler = paddle.io.BatchSampler(dataset,
                                               batch_size=batch_size,
                                               shuffle=shuffle)

    return paddle.io.DataLoader(dataset=dataset,
                                batch_sampler=batch_sampler,
                                collate_fn=batchify_fn,
                                return_list=True)

dev_data_loader = create_dataloader(dev_ds,
                                    mode='dev',
                                    batch_size=batch_size,
                                    batchify_fn=batchify_fn_eval,
                                    trans_fn=trans_func_eval)

class PairwiseMatching(nn.Layer):

    def __init__(self, pretrained_model, dropout=None, margin=0.1):
        super().__init__()
        self.ptm = pretrained_model
        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)
        self.margin = margin

        # hidden_size -> 1, calculate similarity
        self.similarity = nn.Linear(self.ptm.config["hidden_size"], 1)

    @paddle.jit.to_static(input_spec=[
        paddle.static.InputSpec(shape=[None, None], dtype='int64'),
        paddle.static.InputSpec(shape=[None, None], dtype='int64')
    ])
    def predict(self,
                input_ids,
                token_type_ids=None,
                position_ids=None,
                attention_mask=None):

        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,
                                    attention_mask)

        cls_embedding = self.dropout(cls_embedding)
        sim_score = self.similarity(cls_embedding)
        sim_score = F.sigmoid(sim_score)#返回的是相似度分数

        return sim_score

    def forward(self,
                pos_input_ids,
                neg_input_ids,
                pos_token_type_ids=None,
                neg_token_type_ids=None,
                pos_position_ids=None,
                neg_position_ids=None,
                pos_attention_mask=None,
                neg_attention_mask=None):

        _, pos_cls_embedding = self.ptm(pos_input_ids, pos_token_type_ids,
                                        pos_position_ids, pos_attention_mask)

        _, neg_cls_embedding = self.ptm(neg_input_ids, neg_token_type_ids,
                                        neg_position_ids, neg_attention_mask)

        pos_embedding = self.dropout(pos_cls_embedding)
        neg_embedding = self.dropout(neg_cls_embedding)

        pos_sim = self.similarity(pos_embedding)
        neg_sim = self.similarity(neg_embedding)

        pos_sim = F.sigmoid(pos_sim)
        neg_sim = F.sigmoid(neg_sim)

        labels = paddle.full(shape=[pos_cls_embedding.shape[0]],
                             fill_value=1.0,
                             dtype='float32')

        loss = F.margin_ranking_loss(pos_sim,
                                     neg_sim,
                                     labels,
                                     margin=self.margin)

        return loss

margin=0.2

model = PairwiseMatching(pretrained_model, margin=margin)

init_from_ckpt='./checkpoint/yxsy/yysy_best_1.pdparams'

if init_from_ckpt and os.path.isfile(init_from_ckpt):
        state_dict = paddle.load(init_from_ckpt)
        model.set_dict(state_dict)

# preds (numpy.array)：一个numpy数组，形状为(batch_size, 2)。preds[i][j]表示将实例i分类到类别j的概率。
# 这里，j的取值为0或1，分别代表负类和正类。
# labels (numpy.array)：一个numpy数组，形状为(batch_size, 1)。labels[i]是0或1，代表实例i的标签。

@paddle.no_grad()
def evaluate(model, metric, data_loader, phase="dev"):
    model.eval()
    metric.reset()
    for idx, batch in enumerate(data_loader):
        input_ids, token_type_ids, labels = batch

        pos_probs = model.predict(input_ids=input_ids,
                                  token_type_ids=token_type_ids)

        neg_probs = 1.0 - pos_probs

        preds = np.concatenate((neg_probs, pos_probs), axis=1)
        metric.update(preds=preds, labels=labels)

    print("eval_{} auc:{:.3}".format(phase, metric.accumulate()))

metric = paddle.metric.Auc()#auc
evaluate(model, metric, dev_data_loader, "dev")

def read_text_pair(data_path):
    """Reads data."""
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = line.rstrip().split("\t")
            if len(data) != 3:
                continue
            yield {'query': data[0], 'title': data[1]}

trans_func = partial(convert_pairwise_example,
                     tokenizer=tokenizer,
                     max_seq_length=max_seq_length,
                     phase="predict")

paddle.get_device()

batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype="int64"),  # input_ids
        Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype="int64"
            ),  # segment_ids
    ): [data for data in fn(samples)]

input_file='./datasets/yysy/sort/test_pairwise.csv'

valid_ds = load_dataset(read_text_pair,
                            data_path=input_file,
                            lazy=False)

valid_data_loader = create_dataloader(valid_ds,
                                      mode='predict',
                                      batch_size=batch_size,
                                      batchify_fn=batchify_fn,
                                      trans_fn=trans_func)

model = PairwiseMatching(pretrained_model,margin=0.2)

params_path='./checkpoint/yxsy/yysy_best_1.pdparams'

if params_path and os.path.isfile(params_path):
    state_dict = paddle.load(params_path)
    model.set_dict(state_dict)
    print("Loaded parameters from %s" % params_path)
else:
    raise ValueError(
        "Please set --params_path with correct pretrained model file")

def predict(model, data_loader):
    batch_probs = []
    model.eval()
    with paddle.no_grad():
        for batch_data in data_loader:
            input_ids, token_type_ids = batch_data
            batch_prob = model.predict(input_ids=input_ids,#批次相似度
                                       token_type_ids=token_type_ids).numpy()

            batch_probs.append(batch_prob)
        if (len(batch_prob) == 1):
            batch_probs = np.array(batch_probs)
        else:
            batch_probs = np.concatenate(batch_probs, axis=0)

        return batch_probs

for i in valid_data_loader:
    print(i)
    break

y_probs = predict(model, valid_data_loader)

valid_ds = load_dataset(read_text_pair,
                            data_path=input_file,
                            lazy=False)

for idx, prob in enumerate(y_probs):
    text_pair = valid_ds[idx]#文本对
    text_pair["pred_prob"] = prob[0]#prob[0]取出里面的值,是文本对的相似度概率
    print(text_pair)
