import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ["KERAS_BACKEND"] = "tensorflow"
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import keras
from keras import layers
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np

(x_train, y_train), (x_test, y_test) =tf.keras.datasets.fashion_mnist.load_data()

val_split = 0.1

x_train=np.expand_dims(x_train,-1)
x_test=np.expand_dims(x_test,-1)

val_indices = int(len(x_train) * val_split)

new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]

x_val, y_val = x_train[:val_indices], y_train[:val_indices]

batch_size=256

image_size=28

augmentation_layers = [
    # keras.layers.Resizing(30,30),
    keras.layers.RandomCrop(image_size, image_size),#随机裁剪
    keras.layers.RandomFlip("horizontal"),#随机水平翻转
    # keras.layers.RandomRotation(0.06),#随机旋转
    # keras.layers.RandomZoom(0.06)#随机缩放
]
def augment_images(images):
    for layer in augmentation_layers:#连续处理images
        images = layer(images, training=True)
    return images

def make_datasets(images, labels, is_train=False):
    dataset = tf.data.Dataset.from_tensor_slices((images, labels))
    if is_train:
        dataset = dataset.shuffle(batch_size * 100)
    dataset = dataset.batch(batch_size)
    if is_train:
        dataset = dataset.map(
            lambda x, y: (augment_images(x), y), num_parallel_calls=tf.data.AUTOTUNE
        )
    return dataset.prefetch(tf.data.AUTOTUNE)
train_dataset = make_datasets(new_x_train, new_y_train, is_train=True)
val_dataset = make_datasets(x_val, y_val)
test_dataset = make_datasets(x_test, y_test)

def get_conv_pool_model(image_size=28, num_classes=10):
    inputs = keras.Input((image_size, image_size,1))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)#0-1归一化
    for size in (64,128,256):
        # x=layers.BatchNormalization()(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.MaxPool2D()(x)
    x=layers.GlobalAveragePooling2D()(x)
    # x=layers.Flatten()(x)
    x=layers.Dropout(0.5)(x)
    # x=layers.Dense(256,activation='relu')(x)
    logits = layers.Dense(num_classes)(x)
    return keras.Model(inputs,logits)

model=get_conv_pool_model()

learning_rate=1e-3
weight_decay=1e-4

optimizer = keras.optimizers.AdamW(
    learning_rate=learning_rate, weight_decay=weight_decay
)

model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ]
    )

from tensorflow.keras.callbacks import ReduceLROnPlateau

checkpoint_filepath = "./checkpoint/fashion_best_1.keras"
callbacks = [ keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,monitor='val_loss',\
    verbose=1,save_best_only=True,mode='min'),
keras.callbacks.EarlyStopping(monitor="val_loss", patience=4),#超过四次验证损失不减少就停止
ReduceLROnPlateau(monitor='val_loss', factor=0.5,  
                           patience=2, min_lr=1e-5)
]
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=30,
    callbacks=[callbacks]
)

model.load_weights('./checkpoint/fashion_best_1.keras')
_, acc= model.evaluate(test_dataset)
print("Test accuracy: %.2f %%" %(acc))

def activation_block(x):#激活函数,批次标准化块
    x = layers.Activation("gelu")(x)#gelu与relu不同,gelu允许负值存在
    return layers.BatchNormalization()(x)

def get_conv_pool_norm_model(image_size=28, num_classes=10):
    inputs = keras.Input((image_size, image_size,1))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)#0-1归一化
    for size in (64,128,256):
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.MaxPool2D()(x)
        x=activation_block(x)
    x=layers.GlobalAveragePooling2D()(x)
    # x=layers.Flatten()(x)
    x=layers.Dropout(0.5)(x)
    # x=layers.Dense(256,activation='relu')(x)
    logits = layers.Dense(num_classes)(x)
    return keras.Model(inputs,logits)

model2=get_conv_pool_norm_model()

def run_model(model):
    optimizer = keras.optimizers.AdamW(
    learning_rate=learning_rate, weight_decay=weight_decay
    )
    model.compile(
        optimizer=optimizer,
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="acc"),
    ]
    )
    checkpoint_filepath = "./checkpoint/fashion_best_6.keras"
    callbacks = [ keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,monitor='val_loss',\
    verbose=1,save_best_only=True,mode='min'),
    # keras.callbacks.EarlyStopping(monitor="val_loss", patience=4),#超过四次验证损失不减少就停止
    ReduceLROnPlateau(monitor='val_loss', factor=0.5,  
                               patience=2, min_lr=1e-5)
    ]
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=30,
        callbacks=[callbacks]
    )
    return history

history=run_model(model2)

model2.load_weights('./checkpoint/fashion_best_2.keras')
_, acc= model2.evaluate(test_dataset)
print("Test accuracy: %.2f %%" %(acc))

def get_conv_pool_norm2_model(image_size=28, num_classes=10):
    inputs = keras.Input((image_size, image_size,1))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)#0-1归一化
    for size in (64,128,256):
        x=activation_block(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.MaxPool2D()(x)
    x=layers.GlobalAveragePooling2D()(x)
    # x=layers.Flatten()(x)
    x=layers.Dropout(0.5)(x)
    # x=layers.Dense(256,activation='relu')(x)
    logits = layers.Dense(num_classes)(x)
    return keras.Model(inputs,logits)

model3=get_conv_pool_norm2_model()

his=run_model(model3)

model3.load_weights('./checkpoint/fashion_best_2.keras')
_, acc= model3.evaluate(test_dataset)
print("Test accuracy: %.2f %%" %(acc))

def get_conv_pool_norm4_model(image_size=28, num_classes=10):
    inputs = keras.Input((image_size, image_size,1))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)#0-1归一化
    for size in (64,128,256):
        x=layers.BatchNormalization()(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        if size!=256:
            x=layers.MaxPool2D()(x)
        else:
            x=tf.keras.layers.ZeroPadding2D()(x)
            x=layers.MaxPool2D()(x)
    x=layers.GlobalMaxPooling2D()(x)
    x=layers.Dropout(0.5)(x)
    logits = layers.Dense(num_classes)(x)
    return keras.Model(inputs,logits)

model=get_conv_pool_norm4_model()

hsi=run_model(model)

model.load_weights('./checkpoint/fashion_best_5.keras')
_, acc= model.evaluate(test_dataset)
print("Test accuracy: %.2f %%" %(acc))

def get_conv_pool_norm5_model(image_size=28, num_classes=10):
    inputs = keras.Input((image_size, image_size,1))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)#0-1归一化
    for size in (64,128,256):
        x=layers.BatchNormalization()(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        x=layers.Conv2D(size,3,padding='same',activation='gelu')(x)
        if size!=256:
            x=layers.MaxPool2D()(x)
        else:
            x=tf.keras.layers.ZeroPadding2D()(x)
            x=layers.MaxPool2D()(x)
    x=layers.GlobalAveragePooling2D()(x)
    x=layers.Dropout(0.5)(x)
    logits = layers.Dense(num_classes)(x)
    return keras.Model(inputs,logits)

def get_conv_mixer_256_8_2(
    image_size=28, filters=256, depth=8, kernel_size=3, patch_size=2, num_classes=10
):
    inputs = keras.Input((image_size, image_size,1))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)#0-1归一化
    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)
    x = layers.Activation("gelu")(x)
    for _ in range(depth):
        x=layers.BatchNormalization()(x)
        x0 = x
        x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding="same")(x)
        x = layers.Add()([x, x0]) # 残差连接,对应位置数据相加
        x = layers.Conv2D(filters, kernel_size=1)(x)
        x = layers.Activation("gelu")(x)
    # 分类块
    x = layers.GlobalAvgPool2D()(x)#全局平均池化,可以把特征图的特征归一汇总
    x=layers.Dropout(0.5)(x)
    logits = layers.Dense(num_classes)(x)
    return keras.Model(inputs,logits)
