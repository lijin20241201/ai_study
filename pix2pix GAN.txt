import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'#设置tensorflow的日志级别
from tensorflow.python.platform import build_info

import tensorflow as tf
import os
# 用于处理文件系统路径的面向对象的库。pathlib 提供了 Path 类，
#该类表示文件系统路径，并提供了很多方法来操作这些路径。
import pathlib
import time
import datetime
from matplotlib import pyplot as plt
from IPython import display

# 列出所有物理GPU设备  
gpus = tf.config.list_physical_devices('GPU')  
if gpus:  
    # 如果有GPU，设置GPU资源使用率  
    try:  
        # 允许GPU内存按需增长  
        for gpu in gpus:  
            tf.config.experimental.set_memory_growth(gpu, True)  
        # 设置可见的GPU设备（这里实际上不需要，因为已经通过内存增长设置了每个GPU）  
        # tf.config.set_visible_devices(gpus, 'GPU')  
        print("GPU可用并已设置内存增长模式。")  
    except RuntimeError as e:  
        # 虚拟设备未就绪时可能无法设置GPU  
        print(f"设置GPU时发生错误: {e}")  
else:  
    # 如果没有GPU  
    print("没有检测到GPU设备。")

dataset_name = "facades"
path_to_zip  = pathlib.Path('./datasets')
PATH = path_to_zip/dataset_name

list(PATH.iterdir())

sample_image = tf.io.read_file(str(PATH/'train/1.jpg'))# 样本图片,还是二进制

sample_image = tf.io.decode_jpeg(sample_image)
print(sample_image.shape) #高256,宽512彩色图片,因为包含两个子图

plt.figure()
plt.imshow(sample_image)

# 您需要将真实的建筑立面图像与建筑标签图像分开，所有这些图像的大小都是 256 x 256
# 定义加载图像文件并输出两个图像张量的函数
def load(image_file):
  # 读取图片文件,并且解码转换成uint8
  image = tf.io.read_file(image_file)
  image = tf.io.decode_jpeg(image)
  w = tf.shape(image)[1]
  w = w // 2
  input_image = image[:, w:, :]#标签图片
  real_image = image[:, :w, :]#真实图片
  #把两个图片转换成 float32 tensors
  input_image = tf.cast(input_image, tf.float32)
  real_image = tf.cast(real_image, tf.float32)
  return input_image, real_image

# 绘制输入图像（建筑标签图像）和真实（建筑立面照片）图像的样本
#调用定义的load方法加载图片并且预处理
inp, re = load(str(PATH / 'train/100.jpg'))
print(inp.shape,re.shape)
plt.figure()
plt.imshow(inp / 255.0)#归一化
plt.figure()
plt.imshow(re / 255.0)

# 定义几个具有以下功能的函数：
# 将每个 256 x 256 图像调整为更大的高度和宽度，286 x 286。
# 将其随机裁剪回 256 x 256。
# 随机水平翻转图像，即从左到右（随机镜像）。
#将图像归一化到 [-1, 1] 范围。

#缓冲池大小
BUFFER_SIZE = 400
#批次大小
BATCH_SIZE = 1
# 图片宽高
IMG_WIDTH = 256
IMG_HEIGHT = 256

# 最近邻插值是一种简单的插值方法，它选择离目标点最近的像素值作为插值结果。
# 这种方法计算速度快，但可能在图像缩放时引入锯齿状的边缘。如果你需要更平
# 滑的缩放效果，可以考虑使用其他插值方法，如双线性插值（tf.image.ResizeMethod.BILINEAR）
# 或双三次插值（tf.image.ResizeMethod.BICUBIC），改变大小肯定涉及填充放大区域

#改变图片大小
def resize(input_image, real_image, height, width):
  input_image = tf.image.resize(input_image, [height, width],
                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
  real_image = tf.image.resize(real_image, [height, width],
                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
  return input_image, real_image

#定义随机裁剪的方法
# 如果 input_image 和 real_image 在除了批次大小维度之外的其他维度上有不同的形状，tf.stack 函数会抛出错误。
# 因此，在使用 tf.stack 之前，确保你要堆叠的张量在除了堆叠轴之外的所有维度上都有相同的形状是很重要的。
def random_crop(input_image, real_image):
  #先按样本轴堆叠
  stacked_image = tf.stack([input_image, real_image], axis=0)
# 随机裁剪是数据增强（data augmentation）的一种常见技术，它可以帮助模型在训练时看到输入数据的不同变体，从而提高模型的泛化能力。
# 因为裁剪是随机的，所以每次调用 tf.image.random_crop 时，都可能得到不同的裁剪结果。
  cropped_image = tf.image.random_crop(
      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])
  return cropped_image[0], cropped_image[1]

# 标准化图片到 [-1, 1]
def normalize(input_image, real_image):
  input_image = (input_image / 127.5) - 1
  real_image = (real_image / 127.5) - 1
  return input_image, real_image

#转换为tensorflow计算图函数,random_jitter被装饰为TensorFlow的计算图函数，
# 但是否带梯度取决于函数内部的操作。如果函数内部只包含可微分的TensorFlow操作，
# 那么它的输出相对于输入是可微分的，即可以计算梯度。
@tf.function()
def random_jitter(input_image, real_image):
  # 改变尺寸到 286x286
  input_image, real_image = resize(input_image, real_image, 286, 286)
  #对图片做随机裁剪
  input_image, real_image = random_crop(input_image, real_image)
  #生成服从均匀分布的0-1之间随机数，如果随机数>0.5,对两个图片做水平翻转变换
  #有50%的几率做水平翻转
  if tf.random.uniform(()) > 0.5:
    input_image = tf.image.flip_left_right(input_image)
    real_image = tf.image.flip_left_right(real_image)
  return input_image, real_image

plt.figure(figsize=(6, 6))# 显示
for i in range(4):
  rj_inp, rj_re = random_jitter(inp, re)
  plt.subplot(2, 2, i + 1)
  plt.imshow(rj_inp / 255.0)
  plt.axis('off')
plt.show()

#加载本地磁盘文件，并做图片数据增强，标准化
def load_image_train(image_file):
  input_image, real_image = load(image_file)
  input_image, real_image = random_jitter(input_image, real_image)
  input_image, real_image = normalize(input_image, real_image)
  return input_image, real_image

#对测试图片不做数据增强，只改变大小，标准化
def load_image_test(image_file):
  input_image, real_image = load(image_file)
  input_image, real_image = resize(input_image, real_image,
                                   IMG_HEIGHT, IMG_WIDTH)
  input_image, real_image = normalize(input_image, real_image)
  return input_image, real_image

# 使用 tf.data.Dataset.list_files 方法从 PATH / 'train/*.jpg' 指定的路径中列出所有 .jpg 文件的路径。
# str(PATH / 'train/*.jpg') 会生成一个字符串
train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))

#对训练数据做加载,改变尺寸，随机裁剪,以一半的概率水平翻转，之后标准化输出
# num_parallel_calls=tf.data.AUTOTUNE 允许 TensorFlow 自动选择合适的并行调用数量来优化性能。
train_dataset = train_dataset.map(load_image_train,
                                  num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(BUFFER_SIZE)#缓冲区大小，可以随机刷新数据
train_dataset = train_dataset.batch(BATCH_SIZE)#返回指定批次数据

#列出测试数据路径集
try:
  test_dataset = tf.data.Dataset.list_files(str(PATH / 'test/*.jpg'))
except tf.errors.InvalidArgumentError:
  test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))
test_dataset = test_dataset.map(load_image_test)
test_dataset = test_dataset.batch(BATCH_SIZE)

 U-Net 由编码器（下采样器）和解码器（上采样器）
# 编码器中的每个块为：Convolution -> Batch normalization -> Leaky ReLU（做的是分类）
# 解码器中的每个块为：Transposed convolution -> Batch normalization -> Dropout（应用于前三个块）-> ReLU（做的是生成）
# 编码器和解码器之间存在跳跃连接（如在 U-Net 中）

OUTPUT_CHANNELS = 3# 通道(层),3表示3通道，一般是rgb

#定义下采样
def downsample(filters, size, apply_batchnorm=True):
   # 用于创建一个正态分布的初始化器（initializer）。具体来说，这个初始化器
    # 会生成符合均值为 0，标准差为 0.02 的正态分布的随机值
  initializer = tf.random_normal_initializer(0., 0.02)
  result = tf.keras.Sequential()
  # kernel_initializer 参数用于指定用于初始化卷积核（或称为滤波器）的初始化器。
  #filters：指定卷积层中滤波器的数量,size：卷积核的大小,strides=2：卷积操作的步长
  #padding='same'：填充方式，'same' 表示在输入数据的边缘进行填充
  result.add(
      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                             kernel_initializer=initializer, use_bias=False))
  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())
  result.add(tf.keras.layers.LeakyReLU())
  return result

down_model = downsample(3, 4)
down_result = down_model(tf.expand_dims(inp, 0))
print (down_result.shape)

#定义上采样器,#filters：指定卷积层中滤波器的数量,size：卷积核的大小,apply_dropout:是否用dropout
def upsample(filters, size, apply_dropout=False):
  # 用于创建一个正态分布的初始化器（initializer）。
  initializer = tf.random_normal_initializer(0., 0.02)
  result = tf.keras.Sequential()#
  result.add(
    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                    padding='same',
                                    kernel_initializer=initializer,
                                    use_bias=False))
  result.add(tf.keras.layers.BatchNormalization())#这个批次标准化有两个可训练参数
  if apply_dropout:#如果指定用dropout
      result.add(tf.keras.layers.Dropout(0.5))
  result.add(tf.keras.layers.LeakyReLU())
  return result

up_model = upsample(3, 4)
up_result = up_model(down_result)
print (up_result.shape)

#使用下采样和上采样定义生成器
# 展示了U-Net模型的核心结构：下采样以捕获上下文信息，上采样以恢复原始分辨率，
# 并通过跳过连接来融合不同层次的特征信息
def Generator():
  inputs = tf.keras.layers.Input(shape=[256, 256, 3])#输入层
  #下采样序列堆栈,具体到抽象的过程（用来提取有用的特征）
  down_stack = [
    #注意:这个没用BatchNormalization层,下面的都用了,因为apply_batchnorm=True是默认
    downsample(64, 4, apply_batchnorm=False),#(batch_size, 128, 128, 64)
    downsample(128, 4),  # (batch_size, 64, 64, 128)
    downsample(256, 4),  # (batch_size, 32, 32, 256)
    downsample(512, 4),  # (batch_size, 16, 16, 512)
    downsample(512, 4),  # (batch_size, 8, 8, 512)
    downsample(512, 4),  # (batch_size, 4, 4, 512)
    downsample(512, 4),  # (batch_size, 2, 2, 512)
    downsample(512, 4),  # (batch_size, 1, 1, 512)
  ]
  #上采样序列堆栈,前三个用dropout,后面的没用dropout,dropout默认0.5
  up_stack = [
    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2,512)
    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4,512)
    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8,512)
    upsample(512, 4),  # (batch_size, 16, 16,512)
    upsample(256, 4),  # (batch_size, 32, 32,512)
    upsample(128, 4),  # (batch_size, 64, 64,512)
    upsample(64, 4),  # (batch_size, 128, 128,512)
  ]
  initializer = tf.random_normal_initializer(0., 0.02)
  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,
                                         strides=2,
                                         padding='same',
                                         kernel_initializer=initializer,
                                         activation='tanh')  # (batch_size, 256, 256, 3)
  x = inputs
  skips = []
  # 上采样和下采样最后一维的形状有逆对应关系
  for down in down_stack:
    x = down(x)
    skips.append(x)#下采样输出的特征图
  skips = reversed(skips[:-1])#除去最后一个的每个下采样的特征图
  # 把上采样输入形状和下采样输出特征图形状相同的绑定在一起
  #这里x形状是(batch_size, 1, 1, 512)
  for up, skip in zip(up_stack, skips):
    x = up(x)#第一次上采样后形状是(batch_size, 2, 2,512)
    # 合并是通过在最后一个轴（通常是特征轴）上连接两个张量来实现的。
    #还有除了第一次,其他每次上采样的输出接受的参数都是上采样输出和之前下采样特征图合并后形成的输出
    #这样有利于上采样能更精确的恢复原始图片信息
    # 合并是抽象的和抽象的特征合并，具体的和具体的合并，这个过程是抽象到具体的过程
    x = tf.keras.layers.Concatenate()([x, skip])#与skip跳过连接 在最后一个轴合并
    # print(x.shape)
    #这时x形状是((batch_size, 2, 2,1024))
  x = last(x)# (batch_size, 256, 256, 3)
  return tf.keras.Model(inputs=inputs, outputs=x)

import pydot

tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)

gen_output = generator(inp[tf.newaxis, ...], training=False)
plt.imshow(gen_output[0, ...])

# GAN 学习适应数据的损失，而 cGAN 学习结构化损失，该损失会惩罚与网络输出和目标图像不同的可能结构
# 生成器损失是生成图像和一数组的 sigmoid 交叉熵损失
# 论文还提到了 L1 损失，它是生成图像与目标图像之间的 MAE（平均绝对误差）
# 这样可使生成的图像在结构上与目标图像相似
# 计算总生成器损失的公式为：gan_loss + LAMBDA * l1_loss，其中 LAMBDA = 100。该值由论文作者决定

LAMBDA = 100
#二元交叉熵
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
#定义生成器损失函数
def generator_loss(disc_generated_output, gen_output, target):
  #计算(判别器对生成器生成的输出logits)与全1(表示True)之间的差别，用的是二元交叉熵
  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)
  # mae,平均绝对值误差,计算真实和生成器生成的图的区别，用的是mae
  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))
  #这说明ll_loss占比更大，更重要
  total_gen_loss = gan_loss + (LAMBDA * l1_loss)
  return total_gen_loss, gan_loss, l1_loss

# 判别器中的每个块为：Convolution -> Batch normalization -> Leaky ReLU
# 最后一层之后的输出形状为 (batch_size, 30, 30, 1)。
# 输出的每个 30 x 30 图像分块会对输入图像的 70 x 70 部分进行分类
# 判别器接收 2 个输入
# 输入图像和目标图像，应分类为真实图像
# 输入图像和生成图像（生成器的输出），应分类为伪图像。
# 使用tf.concat([inp, tar], axis=-1) 将这 2 个输入连接在一起

#判别器
def Discriminator():
  #用来初始化滤波器
  initializer = tf.random_normal_initializer(0., 0.02)
  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image') #输入图像
  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')#目标图片
  #在最后一个轴合并
  x = tf.keras.layers.concatenate([inp, tar]) #(batch_size, 256, 256, channels*2)
  #下采样，第一次不用batchnormal(有两个可训练参数),默认是应用,apply_batchnorm=True
  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)
  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)
  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)
  # 这行代码的作用是在 down3 这个张量的周围（顶部、底部、左侧和右侧）添加零值的像素。
  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)
  conv = tf.keras.layers.Conv2D(512, 4, strides=1,
                                kernel_initializer=initializer,
                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)
  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)
  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)
  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)
  last = tf.keras.layers.Conv2D(1, 4, strides=1,
                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)
  return tf.keras.Model(inputs=[inp, tar], outputs=last)

discriminator = Discriminator()
tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)

disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)
plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')
plt.colorbar()

#定义判别器损失,参数：判别器给真实图片打的分和判别器给生成器生成图片打的分
def discriminator_loss(disc_real_output, disc_generated_output):
  #判别器给真实图片打的分和全1(true)的比较，用的是二元交叉熵
  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)
  #判别器给生成器生成图片打的分和全0(False)的比较
  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)
  #判别器损失是两者的和,随着训练增加，generated_loss会变大,real_loss会变小
  total_disc_loss = real_loss + generated_loss
  return total_disc_loss

# beta_1是一个超参数，它控制着梯度的一阶矩估计（即指数加权移动平均）的衰减率。
# Adam优化器是自适应学习率优化算法的一种，它结合了Adagrad和RMSprop算法的优点。
# Adam算法使用两个移动平均量来更新权重：一个是梯度的一阶矩估计（均值），另一个是梯度的
# 二阶矩估计（未中心化的方差）。beta_1和beta_2分别控制着这两个移动平均量的衰减率。
# beta_1（通常接近1，如默认值0.9）：控制梯度一阶矩估计的衰减率。一个较高的beta_1值
# 意味着模型会更加依赖过去的梯度，而一个较低的beta_1值则意味着模型会更加关注最近的梯度
# beta_2（通常也接近1，如默认值0.999）：控制梯度二阶矩估计的衰减率
generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

checkpoint_dir = './training_checkpoints'#检查点保存路径
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")# 检查点前缀
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,#生成器优化器
                                 discriminator_optimizer=discriminator_optimizer,#判别器优化器
                                 generator=generator,#生成器
                                 discriminator=discriminator)#判别器

#定义生成图片方法，参数：生成器模型
def generate_images(model, test_input, tar):
  prediction = model(test_input, training=True)
  plt.figure(figsize=(15, 15))#画布大小
  display_list = [test_input[0], tar[0], prediction[0]]#显示第一个
  title = ['Input Image', 'Ground Truth', 'Predicted Image']
  for i in range(3):
    plt.subplot(1, 3, i+1)#1行3列网格图
    plt.title(title[i])#标上标题
    # 转换图片数据值到0-1
    plt.imshow(display_list[i] * 0.5 + 0.5)
    plt.axis('off')
  plt.show()

for example_input, example_target in test_dataset.take(1):#取出1个批次
  generate_images(generator, example_input, example_target)

# 为每个样本输入生成一个输出。
# 判别器接收 input_image 和生成的图像作为第一个输入。第二个输入为 input_image 和 target_image。
# 接下来，计算生成器和判别器损失。
# 随后，计算损失相对于生成器和判别器变量（输入）的梯度，并将其应用于优化器。
# 最后，将损失记录到 TensorBoard。

log_dir="logs/"
summary_writer = tf.summary.create_file_writer(
  log_dir + "fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

@tf.function#图函数,用于进行每一个批次的训练
def train_step(input_image, target, step):
  #生成器梯度带,鉴别器梯度带
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    #根据输入图像标签获取生成器生成的图片,training=True,会更新参数
    gen_output = generator(input_image, training=True)
    #鉴别器接受标签和真实的目标图片获取logits
    disc_real_output = discriminator([input_image, target], training=True)
    #鉴别器接受标签和生成器生成图片获取鉴别器对造假图片的logits
    disc_generated_output = discriminator([input_image, gen_output], training=True)
    #根据判别器对生成器生成图片的鉴别分数和生成器生成图片和真实图片获取生成器损失(看模仿的到不到位)
    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
    #输入是鉴别器对真实图片的分数和鉴别器对生成图片的分数，计算出鉴别器损失
    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)
  #获取生成器总损失对生成器可训练参数的梯度
  generator_gradients = gen_tape.gradient(gen_total_loss,
                                          generator.trainable_variables)
  #获取鉴别器损失对鉴别器可训练参数的梯度
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               discriminator.trainable_variables)
  #用生成器梯度更新生成器可训练参数
  generator_optimizer.apply_gradients(zip(generator_gradients,
                                          generator.trainable_variables))
  #用鉴别器梯度更新鉴别器可训练参数
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              discriminator.trainable_variables))
  with summary_writer.as_default():
    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)
    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)
    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)
    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)

# 实际的训练循环。由于本教程可以运行多个数据集，并且数据集的大小差异很大，因此将训练循环设置为按步骤而非按周期工作
# 迭代步骤数。
# 每 10 步打印一个点 (.)。
# 每 1 千步：清除显示并运行 generate_images 以显示进度。
# 每 5 千步：保存一个检查点

#定义训练方法：参数：训练数据集，测试数据集，步
def fit(train_ds, test_ds, steps):
  example_input, example_target = next(iter(test_ds.take(1)))#获取1个批次
  start = time.time()#起始时间
  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():
    if (step) % 1000 == 0: #1000步清除之前输出,重新计时
      display.clear_output(wait=True)
      if step != 0:
        print(f'训练1000步的时间: {time.time()-start:.2f} 秒\n')
      start = time.time()#重设起始时间
      #画图
      generate_images(generator, example_input, example_target)
      print(f"当前步数(k): {step//1000}k")
    train_step(input_image, target, step) #训练，梯度更新
    # Training step
    if (step+1) % 10 == 0:#10步一点
      print('.', end='', flush=True)
    # 5000步保存检查点
    if (step + 1) % 5000 == 0:
      checkpoint.save(file_prefix=checkpoint_prefix)

# 此训练循环会保存日志，您可以在 TensorBoard 中查看这些日志以监控训练进度。
# 如果您使用的是本地计算机，则需要启动一个单独的 TensorBoard 进程。在jupyter中工作时，
# 请在开始训练之前启动查看器以使用 TensorBoard 进行监控。

# %load_ext tensorboard：这一行代码加载了TensorBoard的Jupyter Notebook扩展。
# 这允许你在Jupyter Notebook中直接使用TensorBoard的功能
# %tensorboard --logdir {logs}：这一行代码启动TensorBoard并设置其日志目录为{logs}。
# {logs}是一个占位符，你需要将其替换为你实际的日志目录路径。这个日志目录应该包含TensorFlow
# 在训练过程中产生的事件文件（通常以.tfevents为后缀）,这些文件记录了训练过程中的各种信息，如损失、准确率等
%load_ext tensorboard
%tensorboard --logdir {log_dir}

fit(train_dataset, test_dataset, steps=25000)
