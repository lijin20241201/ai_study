import os
import random
import time
import math
from functools import partial
import numpy as np
import paddle
from paddle.io import DataLoader
from paddlenlp.transformers import LinearDecayWithWarmup
from paddlenlp.metrics import ChunkEvaluator
from datasets import load_dataset
from paddlenlp.transformers import BertForTokenClassification, BertTokenizer
from paddlenlp.transformers import ErnieForTokenClassification, ErnieTokenizer
from paddlenlp.transformers import ErnieCtmForTokenClassification, ErnieCtmTokenizer
from paddlenlp.data import DataCollatorForTokenClassification

from paddlenlp.utils.log import logger

MODEL_CLASSES = {
    "bert": (BertForTokenClassification, BertTokenizer),#bert
    "ernie": (ErnieForTokenClassification, ErnieTokenizer),#ernie
    "ernie-ctm": (ErnieCtmForTokenClassification, ErnieCtmTokenizer)#ernie-ctm
}

model_type='ernie'
model_name_or_path='ernie-3.0-base-zh'
dataset='msra_ner'
output_dir='./checkpoints/msra_ner'
max_seq_length=128
batch_size=10
learning_rate=5e-5
weight_decay=0.0
adam_epsilon=1e-8
max_grad_norm=1.0
num_train_epochs=3
warmup_steps=0
logging_steps=10
seed=1000
device='gpu'

a=[list(list(MODEL_CLASSES.values()) [1][-1].pretrained_init_configuration.keys())]

", ".join(sum(a,[]))#列表内必须也是列表，才能sum

paddle.set_device(device)

raw_datasets = load_dataset(dataset)

AutoForTokenClassification, AutoTokenizer = MODEL_CLASSES[model_type]

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

train_ds = raw_datasets['train']

label_list = train_ds.features['ner_tags'].feature.names

train_ds.features['ner_tags']

label_num = len(label_list)

no_entity_id = 0

def tokenize_and_align_labels(examples):#分词和对齐标签
        tokenized_inputs = tokenizer(
            examples['tokens'],
            max_seq_len=max_seq_length,
            is_split_into_words='token',
            return_length=True)
        labels = []
        for i, label in enumerate(examples['ner_tags']):
            label_ids = label
            if len(tokenized_inputs['input_ids'][i]) - 2 < len(label_ids):
                label_ids = label_ids[:len(tokenized_inputs['input_ids'][i]) - 2]
            label_ids = [no_entity_id] + label_ids + [no_entity_id]
            label_ids += [no_entity_id] * ( len(tokenized_inputs['input_ids'][i]) - len(label_ids))
            labels.append(label_ids)
        tokenized_inputs["labels"] = labels
        return tokenized_inputs

 train_ds = train_ds.select(range(len(train_ds) - 1))

column_names = train_ds.column_names

train_ds = train_ds.map(tokenize_and_align_labels,
                        batched=True,
                        remove_columns=column_names)

ignore_label = -100
batchify_fn = DataCollatorForTokenClassification(
    tokenizer=tokenizer, label_pad_token_id=ignore_label)

train_batch_sampler = paddle.io.DistributedBatchSampler(
        train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
train_data_loader = DataLoader(dataset=train_ds,
                               collate_fn=batchify_fn,
                               num_workers=0,
                               batch_sampler=train_batch_sampler,
                               return_list=True)

test_ds = raw_datasets['test']

test_ds = test_ds.select(range(len(test_ds) - 1))

test_ds= test_ds.map(tokenize_and_align_labels,
                      batched=True,
                      remove_columns=column_names)

test_data_loader = DataLoader(dataset=test_ds,
                              collate_fn=batchify_fn,
                              num_workers=0,
                              batch_size=batch_size,
                              return_list=True)

model = AutoForTokenClassification.from_pretrained(model_name_or_path, num_classes=label_num)

num_training_steps =  len(train_data_loader) * num_train_epochs

lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,
                                     warmup_steps)

decay_params = [
    p.name for n, p in model.named_parameters()
    if not any(nd in n for nd in ["bias", "norm"])
]
optimizer = paddle.optimizer.AdamW(
    learning_rate=lr_scheduler,
    epsilon=adam_epsilon,
    parameters=model.parameters(),
    weight_decay=weight_decay,
    apply_decay_param_fun=lambda x: x in decay_params)# 刨除 "bias", "norm"

loss_fn= paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)
metric = ChunkEvaluator(label_list=label_list)

@paddle.no_grad()
def evaluate(model, loss_fn, metric, data_loader, mode="valid"):
    model.eval()
    metric.reset()
    total_loss, precision, recall, f1_score = 0, 0, 0, 0
    for batch in data_loader:
        logits = model(batch['input_ids'], batch['token_type_ids'])
        loss = loss_fn(logits, batch['labels'])# loss
        preds =paddle.argmax(logits,axis=-1)#模型预测标签索引
        num_infer_chunks, num_label_chunks, num_correct_chunks = metric.compute(
            batch['seq_len'], preds, batch['labels'])
        metric.update(num_infer_chunks.numpy(), num_label_chunks.numpy(),
                      num_correct_chunks.numpy())
        total_loss+=loss.item()
    precision, recall, f1_score = metric.accumulate()
    avg_loss=total_loss/len(data_loader)
    print("%s: eval loss: %f, precision: %f, recall: %f, f1: %f" %
          (mode, avg_loss, precision, recall, f1_score))
    metric.reset()
    model.train()
    return f1_score

eval_steps=500

global_step = 0
best_f1=0.0
tic_train = time.time()
for epoch in range(1,num_train_epochs+1):
    model.train()
    for step, batch in enumerate(train_data_loader,start=1):
        global_step += 1
        logits = model(batch['input_ids'], batch['token_type_ids'])
        loss = loss_fn(logits, batch['labels'])
        if global_step %logging_steps == 0:
            print( "global step %d, epoch:%d,batch: %d, loss: %f, speed: %.2f step/s,best_f1:%.4f"
                % (global_step,epoch,step, loss.item(), logging_steps /
                   (time.time() - tic_train),best_f1))
            tic_train = time.time()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.clear_grad()
        if global_step % eval_steps==0 :
            f1_score=evaluate(model, loss_fn, metric, test_data_loader,"test")
            if f1_score>best_f1:
                best_f1=f1_score
                paddle.save( model.state_dict(),os.path.join(output_dir,'best_msra_ner.pdparams'))

init_checkpoint_path='./checkpoints/msra_ner/best_msra_ner.pdparams'

model_dict = paddle.load(init_checkpoint_path)
model.set_dict(model_dict)

evaluate(model,loss_fn,metric,test_data_loader)

def parse_decodes(input_words, id2label, decodes, lens):
    decodes = [x for batch in decodes for x in batch]
    lens = [x for batch in lens for x in batch]
    outputs = []
    for idx, end in enumerate(lens):
        sent = "".join(input_words[idx]['tokens'])
        tags = [id2label[x] for x in decodes[idx][1:end]]
        sent_out = []
        tags_out = []
        words = ""
        for s, t in zip(sent, tags):
            if t.startswith('B-') or t == 'O':
                if len(words):
                    sent_out.append(words)
                if t.startswith('B-'):
                    tags_out.append(t.split('-')[1])
                else:
                    tags_out.append(t)
                words = s
            else:
                words += s
        if len(sent_out) < len(tags_out):
            sent_out.append(words)
        outputs.append(''.join(
            [str((s, t)) for s, t in zip(sent_out, tags_out)]))
    return outputs

id2label = dict(enumerate(label_list))#转换list为dict

model.eval()
pred_list = []
len_list = []
for step, batch in enumerate(test_data_loader):
    logits = model(batch['input_ids'], batch['token_type_ids'])
    pred = paddle.argmax(logits, axis=-1)#预测的标签索引
    # print(pred.shape)#(batch_size,seq_len)
    pred_list.append(pred.numpy())
    len_list.append(batch['seq_len'].numpy())

predict_examples = load_dataset('msra_ner',split=('test'))

predict_examples = predict_examples.select(range(len(predict_examples) - 1))

preds = parse_decodes(predict_examples, id2label, pred_list, len_list)

file_path = "results.txt"
with open(file_path, "w", encoding="utf8") as fout:
    fout.write("\n".join(preds))
print(
    "The results have been saved in the file: %s, some examples are shown below: "
    % file_path)
print("\n".join(preds[:5]))


————————————————

                            版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接和本声明。
                        
原文链接：https://blog.csdn.net/LIjin_1006/article/details/140212109
